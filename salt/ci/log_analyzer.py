#!/usr/bin/env python2
'''
Script helps split the big stdout log file (which is generated by
integration.py) and search for specific message.
'''

__author__ = 'Hung Nguyen Viet'
__maintainer__ = 'Hung Nguyen Viet'
__email__ = 'hvn@robotinfra.com'

import argparse
import os
import shutil
import subprocess as spr
import logging
import tempfile


logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

SKIPS = [
    "OrderedDict",
    "Results of YAML rendering",
    "salt.utils.jinja",
]


def link_to_localpath(link):
    '''
    Turn
    https://ci.robotinfra.com/job/Chunks/292/artifact/Chunks-stdout.log.xz
    To
    /var/lib/jenkins/jobs/Chunks/builds/292/archive/Chunks-stdout.log.xz
    '''
    s = link.split('/job/')[1]
    s = '/builds/'.join(s.split('/', 1))
    return os.path.join('/var/lib/jenkins/jobs',
                        s.replace('/artifact/', '/archive/'))


def filtered_logs(logfile, slsname):
    '''
    a python version of sed '/Run top: slsname/Run top:/'
    This depends on the log of integration.py, change it accordingly if
    integration.py changes.
    '''
    start_of_run_sls = 'Run top: {0}'.format(slsname)
    start_of_next_sls = 'Run top:'

    foundsls = False
    with open(logfile) as origfile:
        write = False
        for line in origfile:
            if write and start_of_next_sls in line:
                # it is writing and meet the next sls log, stop
                break
            if start_of_run_sls == line:
                foundsls = True
                logger.debug("Start writing log relate to %s", slsname)
                write = True
            if write:
                yield line

    if not foundsls:
        logger.warning("Found no log for SLS: %s", slsname)


def to_skip(line):
    for s in SKIPS:
        if s in line:
            return True
    return False


def output_log_nearby(log_generator, errormsg, count=7):
    beforerr = None
    for line in log_generator:
        if to_skip(line):
            continue
        if errormsg in line:
            print beforerr
            print line
            # print next lines
            for i, line in enumerate(log_generator):
                if to_skip(line):
                    continue
                print line
                if i == count:
                    return
        beforerr = line


def unxz(xzfile):
    '''
    Make a copy of xzfile, and extract it to be able to choose destination
    dir for extracting.
    Returns path of extracted file.
    '''
    tempdir = tempfile.mkdtemp()
    logger.debug("Copying from %s to %s", xzfile, tempdir)
    shutil.copy(xzfile, tempdir)
    xzcopy = os.path.join(tempdir, os.path.basename(xzfile))
    spr.call(['unxz', xzcopy])
    return os.path.splitext(xzcopy)[0]


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        'logfile',
        help='path of log file',
    )
    parser.add_argument('sls',
                        help='name of sls contains error, uses . as path sep')
    parser.add_argument('errorlog',
                        help='error log'
                        )
    parser.add_argument('-C', help='Output dir')

    args = parser.parse_args()

    if args.logfile.startswith('http'):
        args.logfile = link_to_localpath(args.logfile)

    logtxt = args.logfile
    if args.logfile.endswith('.xz'):
        logtxt = unxz(args.logfile)
        logger.debug("Extracted, got: %s", logtxt)

    ofile = logtxt + ".short.log"

    if args.C:
        outfn = os.path.basename(logtxt)
        ofile = os.path.join(args.C, outfn)

    with open(ofile, 'w+') as of:
        for line in filtered_logs(logtxt, args.sls):
            of.write(line)

    # only remove log file if it has been extracted from xz archive
    if args.logfile.endswith('.xz'):
        os.remove(logtxt)
        logger.info("Deleted file %s", logtxt)

    with open(ofile) as f:
        output_log_nearby(f, args.errorlog)


if __name__ == "__main__":
    main()
